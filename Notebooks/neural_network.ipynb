{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 616,
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from joblib import load\n",
    "\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "from tempfile import TemporaryDirectory\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from torcheval.metrics.functional import binary_auprc\n",
    "\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from config import DATA_FEATURE_ENGINEERED_DIR, DATA_TRIAL_DIR"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data from Pickle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "outputs": [],
   "source": [
    "def load_data(trial: bool, feature_idx: list):\n",
    "    \"\"\" Load Feature Engineered tensor data in .pkl file, applies train-test split. \"\"\"\n",
    "\n",
    "    # Read Pickle Data\n",
    "    data_dir = DATA_TRIAL_DIR if trial else DATA_FEATURE_ENGINEERED_DIR\n",
    "    X = load(data_dir / Path(r\"X_fe.pkl\"))\n",
    "    y = load(data_dir / Path(r\"y_fe.pkl\"))\n",
    "\n",
    "    # Train test split (80% Train, 20% Test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Remove unused features\n",
    "    retained_features_idx = torch.tensor(feature_idx)\n",
    "    X_train = torch.index_select(X_train, 3, retained_features_idx)\n",
    "    X_test = torch.index_select(X_test, 3, retained_features_idx)\n",
    "\n",
    "    # Set -1 values in fire mask and next-day fire mask to 0\n",
    "    X_train[:, :, :, 10] = X_train[:, :, :, 10].clamp(min=0, max=1)\n",
    "    X_test[:, :, :, 10] = X_test[:, :, :, 10].clamp(min=0, max=1)\n",
    "    y_train[:, :, :, 0] = y_train[:, :, :, 0].clamp(min=0, max=1)\n",
    "    y_test[:, :, :, 0] = y_test[:, :, :, 0].clamp(min=0, max=1)\n",
    "\n",
    "    # Permute to adhere to Pytorch Standard (Batch, Channels, Height, Width)\n",
    "    X_train, X_test, y_train, y_test = [t.permute(0,3,1,2) for t in [X_train, X_test, y_train, y_test]]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = load_data(\n",
    "    trial=True,\n",
    "    feature_idx = [0,2,3,4,5,6,7,8,9,10,11,12,14]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create Dataset Class\n",
    "Let's define the Dataset Class for training and testing. The validation set gets generated later from the training sets. We define custom datasets inheriting from torch.utils.data.Dataset to create DataLoaders later"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, X, y, transform=None):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "    self.transform = transform\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    # Return input and target value for the given index\n",
    "    X, y = self.X[index,:,:,:], self.y[index,:,:,:]\n",
    "\n",
    "    if self.transform:\n",
    "        # Concatenate X and y to apply same random transforms and then split them again\n",
    "        X_y = torch.cat((X, y), dim=0)\n",
    "        X_y = self.transform(X_y)\n",
    "\n",
    "        X = X_y[:13, :, :]\n",
    "        y = X_y[13:14, :, :]\n",
    "\n",
    "    return X, y\n",
    "\n",
    "  def __len__(self):\n",
    "    # Return the length of this dataset\n",
    "    return len(self.X)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "outputs": [],
   "source": [
    "# Define Transformations. We use a random horizontal and vertical flip followed by a random 32x32\n",
    "# crop do mitigate our model from predicting fires just in the image center.\n",
    "\n",
    "# Create sequence of means and standard deviations for each of the 13 channels\n",
    "mean_stats = X_train.mean(dim=(0, 2, 3), keepdim=True).squeeze().tolist()\n",
    "std_stats = X_train.std(dim=(0, 2, 3), keepdim=True).squeeze().tolist()\n",
    "\n",
    "# Fire mask should stay binary\n",
    "mean_stats[10], std_stats[10] = 0, 1\n",
    "mean_stats.append(0)\n",
    "std_stats.append(1)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomCrop(32),\n",
    "        transforms.Normalize(mean=mean_stats,\n",
    "                             std=std_stats)\n",
    "        ])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    transform=transform)\n",
    "\n",
    "test_dataset = CustomDataset(\n",
    "    X_test,\n",
    "    y_test,\n",
    "    transform=transform)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define Transform and create DataLoaders"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size= 32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size= 32, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "outputs": [],
   "source": [
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet18\",\n",
    "    encoder_weights=\"imagenet\",\n",
    "    in_channels=13,\n",
    "    classes=1,\n",
    "    activation = 'sigmoid'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "outputs": [],
   "source": [
    "def train_model(train_loader, model, num_epochs: int, criterion, optimizer):\n",
    "\n",
    "    model.to(device)\n",
    "    since = time.time()\n",
    "\n",
    "    # Split train_loader into training and validation sets\n",
    "    data_size = {'train': int(0.8 * len(train_loader.dataset)),\n",
    "                 'val': len(train_loader.dataset) - int(0.8 * len(train_loader.dataset))}\n",
    "\n",
    "    train_dataset, val_dataset = random_split(train_loader.dataset, [data_size['train'], data_size['val']])\n",
    "\n",
    "    dataloaders = {'train': DataLoader(train_dataset, batch_size=train_loader.batch_size, shuffle=True),\n",
    "                   'val': DataLoader(val_dataset, batch_size=train_loader.batch_size, shuffle=False)}\n",
    "\n",
    "\n",
    "    # Create a temporary directory to save training checkpoints\n",
    "    with TemporaryDirectory() as tempdir:\n",
    "        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n",
    "\n",
    "        torch.save(model.state_dict(), best_model_params_path)\n",
    "        best_acc = 0.0\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Training and Validation phase\n",
    "            for phase in ['train', 'val']:\n",
    "                model.train() if phase == 'train' else model.eval()\n",
    "                running_loss, running_auprc = 0.0, 0\n",
    "\n",
    "                # Iterate over data.\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device).squeeze()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # Forward\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        outputs = model(inputs).squeeze()\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # Backward and optimization\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # Statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "                    # For Accuracy use AUPRC as we have unbalanced data\n",
    "                    outputs = (outputs >= 0.5).float()\n",
    "                    outputs_flat = outputs.view(outputs.shape[0], -1)\n",
    "                    labels_flat = labels.view(labels.shape[0], -1)\n",
    "\n",
    "                    auprc = binary_auprc(outputs_flat, labels_flat, num_tasks=len(labels))\n",
    "                    running_auprc += torch.sum(auprc)\n",
    "\n",
    "                epoch_loss = running_loss / data_size[phase]\n",
    "                epoch_auprc = running_auprc.double() / data_size[phase]\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} AUPRC: {epoch_auprc:.4f}')\n",
    "\n",
    "                # Deep copy the model\n",
    "                if phase == 'val' and epoch_auprc > best_acc:\n",
    "                    best_acc = epoch_auprc\n",
    "                    torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "        print(f'Best val Acc: {best_acc:4f}')\n",
    "\n",
    "        # Load best model weights\n",
    "        model.load_state_dict(torch.load(best_model_params_path))\n",
    "\n",
    "        # Train the best model again on all training data without validation\n",
    "        model.train()\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "eta = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=eta)\n",
    "\n",
    "# Define the loss function with weighted positive samples\n",
    "positive_weight = 2.0\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor(positive_weight))\n",
    "\n",
    "model_trained = train_model(train_loader=train_dataloader,\n",
    "                            model=model,\n",
    "                            num_epochs=num_epochs,\n",
    "                            criterion=criterion,\n",
    "                            optimizer=optimizer\n",
    "                            )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model Testing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "outputs": [],
   "source": [
    "def test_model(model, loader):\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    true = []\n",
    "\n",
    "    # Iterate over the test data\n",
    "    with torch.no_grad(): # We don't need to compute gradients for testing\n",
    "        for x_batch, t in loader:\n",
    "            x_batch = x_batch.to(device)\n",
    "            predicted = model(x_batch)\n",
    "            predicted = predicted.cpu().numpy()\n",
    "            predicted[predicted > 0.5] = 1\n",
    "            predicted[predicted <= 0.5] = 0\n",
    "            predictions.append(predicted)\n",
    "            true.append(t.cpu().numpy())\n",
    "\n",
    "    return predictions, true"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "outputs": [],
   "source": [
    "pred, true = test_model(model_trained, test_dataloader)\n",
    "pred = pred[0]\n",
    "true = true[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "    # Plot the true image\n",
    "    axs[0].imshow(true[i, 0], cmap='gray')\n",
    "    axs[0].set_title('True')\n",
    "    axs[0].axis('off')\n",
    "\n",
    "    # Plot the predicted image\n",
    "    axs[1].imshow(pred[i, 0], cmap='gray')\n",
    "    axs[1].set_title('Predicted')\n",
    "    axs[1].axis('off')\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def compute_recall_precision(pred, true):\n",
    "    # Flatten the arrays for easier comparison\n",
    "    pred_flat = torch.FloatTensor(pred).flatten()\n",
    "    true_flat = torch.FloatTensor(true).flatten()\n",
    "\n",
    "    # Compute True Positives, False Positives, and False Negatives\n",
    "    TP = np.sum(np.logical_and(pred_flat == 1, true_flat == 1))\n",
    "    FP = np.sum(np.logical_and(pred_flat == 1, true_flat == 0))\n",
    "    FN = np.sum(np.logical_and(pred_flat == 0, true_flat == 1))\n",
    "    TN = np.sum(np.logical_and(pred_flat == 0, true_flat == 0))\n",
    "\n",
    "    # Compute Accuracy, Recall, and Precision\n",
    "    recall = TP / (TP + FN)\n",
    "    precision = TP / (TP + FP)\n",
    "\n",
    "    return recall, precision\n",
    "\n",
    "\n",
    "recall, precision = compute_recall_precision(pred, true)\n",
    "\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Precision:\", precision)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
